

## Results and figures:


![Figure tsne1000](https://github.com/user-attachments/assets/8a2eaa36-dd5f-4140-a04e-ef019cadea41)
tsne with 1000 vectors:

![Figure kmeans100_000](https://github.com/user-attachments/assets/ef8515cb-3c7f-4ff7-aaf7-e94351c3c298)
kmeans with 100k vectors:

# WIKIPEDIA
Dataset: https://huggingface.co/datasets/wikimedia/wikipedia

## Objective: 

The ojectives of this project, other than learning the management of big data via Spark:

- **Doc2Vec** to vector Wikipedia articles.

- **CLUSTER** articles via *t-SNE*.  

- **CHANGE** *Vector_size* and *epoch* optimizing time and space.

- **PREDICT** vectors with shallow model.

- **SAVE** model to reduce time.

- **LOAD** model to predict new vectors.

- **CREATE** *Recommendation System* for articles.


### Regarding Extra Steps:

This are some steps that can be useful to apply to the project, but are time consuming:

-   **TRAIN** replaced data.

-   **ADD** *sentiment analyisis*, *length of the articles*, *word frequency*...

-   **NORMAL** text, running -> run

